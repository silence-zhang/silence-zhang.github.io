# Practical Deep Learning

Here's the table of contents:

- TOC
{:toc}

## Deep Learning Vocabulary

![p4](/images/Pastedimage20240315144530.png)
![p2](/images/WX20240326-103456@2x.png)

## Machine Learning(ML)

The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.

## Neural Network(NN)

A neural network is a system of artificial neurons that are connected to each other in a way that allows them to learn and recognize patterns.

![p1](/images/Pastedimage20240314180458.png)

Basic Neural Network Equation:

```python

def simple_net(xb):
    res = xb@w1 + b1
    res = res.max(tensor(0.0))
    res = res@w2 + b2
    return res

# takeing advantage of pyTorch
simple_net = nn.Sequential(
    nn.Linear(28*28,30),
    nn.ReLU(),
    nn.Linear(30,1)
)

```

### [Activation Function](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)

The **activation function** known as *transfer function* is another function that is part of the neural network, which has the purpose of providing non-linearity to the model. The idea is that without an activation function, we just have multiple linear functions of the form y=mx+b. However, a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data. By introducing a non-linearity in between the linear layers, this is no longer true. Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions. In fact, it can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights. This is known as the universal approximation theorem.

The Activation Functions can be basically divided into 2 types:

- Linear Activation Functions
- Non-Linear Activation Functions

### Jargon Recap

*Activations*: Numbers that are calculated (both by linear and nonlinear layers)
*Parameters*:  Numbers that are randomly initialized, and optimized (that is, the numbers that define the model)
The number of dimensions of a tensor is its rank. There are some special tensors:

- Rank-0: scalar (0-dimensional tensor)
- Rank-1: vector (1-dimensional tensor)
- Rank-2: matrix (2-dimensional tensor)

## Parallel Distributed Processing(PDP)

- A set of processing units
- A state of activation
- An output function for each unit
- A pattern of connectivity among units
- A propagation rule for propagating patterns of activities through the network of connectivities
- An activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit
- A learning rule whereby patterns of connectivity are modified by experience
- An environment within which the system must operate

## Stochastic Gradient Descent([SGD: 随机梯度下降](https://zh.d2l.ai/chapter_optimization/sgd.html))

Neural networks are special because they are highly flexible, which means they can solve an unusually wide range of problems just by finding the right weights. This is powerful, because stochastic gradient descent provides us a way to find those weight values automatically.

**Seven Steps in SGD:**

1. Initialize the parameters – Random values often work best.
1. Calculate the predictions – This is done on the training set, one mini-batch at a time.
1. Calculate the loss – The average loss over the minibatch is calculated
1. Calculate the gradients – this is an approximation of how the parameters need to change in order to minimize the loss function
1. Step the weights – update the parameters based on the calculated weights
1. Repeat the process
1. Stop – In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.

![p1](/images/WX20240322-120021@2x.png)

### Backward pass and Forward pass

The forward pass is the process of feeding data through the network, and the backward pass is the process of propagating errors back through the network.

### Learning Rate and Stepping

The learning rate is often a number between 0.001 and 0.1. It’s the amount by which the weights are adjusted during each iteration of the training loop.
Once you’ve picked a learning rate, you can adjust your parameters using this simple function:

``` python
w -= w.grad * lr
```

This is known as stepping your parameters, using an optimization step.

![p](/images/WX20240322-151036@2x.png)

![p](/images/WX20240322-151049@2x.png)

![p](/images/WX20240322-151105@2x.png)

### The MNIST Loss Function

The loss function will return a value based on the given predictions and targets, where lower values correspond to better model predictions.

``` python
def mnist_loss(preds, targs):
    preds = preds.sigmoid()
    return torch.where(targs==1, 1-preds, preds).mean()
```

In plain English, this function will measure how distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it should be 0, and then it will take the mean of all those distances.

### Matrix Multiplication

In Python, matrix multiplication is represented with the @ operator.

``` python
Matrix multiplication in Python:
# A is a 2x3 matrix, B is a 3x4 matrix, C is a 2x4 matrix
C = A @ B
```

![p5](/images/WX20240325-144319@2x.png)

### Sigmoid Function

The sigmoid function always outputs a number between 0 and 1.

``` python
def sigmoid(x): return 1 / (1 + torch.exp(-x))
```

![p7](/images/WX20240325-151716.png)

### Rectified Linear Unit([ReLU](https://builtin.com/machine-learning/relu-activation-function))

The *Rectified Linear Unit* is a function that rectifies the output of a neuron. It is a non-linear activation function that is used in neural networks.

```python
# torch built-in function
plot_function(F.relu)
```

![p4](/images/WX20240325-175443.png)

### Mini-Batches

We calculate the average loss for a few data items at a time. This is called a mini-batch. The number of data items in the mini-batch is called the batch size.
The larger batch size, the more accurate the gradient descent, but it will take longer, and you will process fewer mini-batches per epoch.

### Optimizer

Original case:

```python

def mnist_loss(preds, targs):
    preds = preds.sigmoid()
    return torch.where(targs==1, 1-preds, preds).mean()

def calc_grad(xb, yb, model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()

def batch_accuracy(xb, yb):
    preds = xb.sigmoid()
    correct = (preds > 0.5) == yb
    return correct.float().mean()

def validate_epoch(model):
    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]
    return round(torch.stack(accs).mean().item(), 4)

linear_model = nn.Linear(28*28,1)
class BasicOptim:
    def __init__(self, params, lr): self.params, self.lr = list(params), lr
    def step(self, *args, **kwargs):
        for p in self.params: p.data -= p.grad.data * self.lr
    def zero_grad(self, *args, **kwargs):
        for p in self.params: p.grad = None

opt = BasicOptim(linear_model.parameters(), lr)

def train_epoch(model):
    for xb, yb in dl:
        calc_grad(xb, yb, model)
        opt.step()
        opt.zero_grad()

def train_model(model, epochs):
    for i in range(epochs):
        train_epoch(model)
        print(validate_epoch(model), end=' ')

train_model(linear_model, 20)

```

SGD case:

```python
linear_model = nn.Linear(28*28, 1)
opt = SGD(linear_model.parameters(), lr)
train_model(linear_model, 20)
```

Learner case:

```python
dls = DataLoaders(dl, valid_dl)
learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)
learn.fit(10, lr = lr)

```

Network case:

```python
simple_net = nn.Sequential(
    nn.Linear(28*28,30),
    nn.ReLU(),
    nn.Linear(30,1)
)

learn = Learner(dls, simple_net, opt_func=SGD,
 loss_func=mnist_loss, metrics=batch_accuracy)

learn.fit(40,0.1)

```

At this point, we have something that is rather magical:

- A function that can solve any problem to any level of accuracy (the neural net‐work) given the correct set of parameters
- A way to find the best set of parameters for any function (stochastic gradient descent)

## Loss Functions

- nn.CrossEntropyLoss for single-label classification
- nn.BCEWithLogitsLoss for multi-label classification
- nn.MSELoss for regression

## Training Loop

![p2](/images/Pastedimage20240315111753.png)

## Feedback Loops

- A *`predictive policing`* model is created based on where arrests have been made in the past. In practice, this is not actually predicting crime, but rather predicting arrests, and is therefore partially simply reflecting biases in existing policing processes.
- Law enforcement officers then might use that model to decide where to focus their policing activity, resulting in increased arrests in those areas.
- Data on these additional arrests would then be fed back in to retrain future versions of the model.

## Classification and Regression

A classification model is one that attempts to predict a class, or category. That is, it’s predicting from a number of discrete possibilities, such as “dog” or “cat.”
A regression model is one that attempts to predict one or more numeric quantities, such as a temperature or a location.

## Overfitting and Underfitting

confirmed that overfitting is occurring (i.e., if you have observed the validation accuracy getting worse during training).

![p3](/images/Pastedimage20240315115211.png)

## Convolutional Neural Network (CNN)

## Pre-trained Model

A model that has weights that have already been trained on another dataset is called a *pre‐trained* model.

## Transfer Model

Using a pretrained model for a task different from what it was originally trained for.

## Fine-Tuning(adapt a pre-trained model for a new dataset)

A transfer learning technique that updates the parameters of a pre‐ trained model by training for additional epochs using a different task from that used for pretraining.

## Training Sets && Validation Sets && Test Sets

- Training data is fully exposed
- Validation data is less exposed
- Test data is totally hidden

## Independent Variables && Dependent Variables

The independent variable is the thing we are using to make predictions from, and the dependent variable is our target.

## Data Augmentation

Data augmentation refers to creating random variations of our input data, such that they appear different but do not change the meaning of the data.

## Out-Of-Domain data

That is to say, there may be data that our model sees in production that is very different from what it saw during training.

## Domain Shift

This is when the type of data changes gradually over time. For example, an insurance company is using a deep learning model as part of their pricing algorithm, but over time their customers will be different, with the original training data not being representative of current data, and the deep learning model being applied on effectively out-of-domain data.

## Confusion Matrix

![p4](/images/Pastedimage20240320104843.png)

## Data Ethics

- *Recourse and accountability*(追责和问责)
- *Feedback loops*
- *Bias*
- *Disinformation*

## L1 norm and L2 norm and MSE(mean square error)

The L1 norm is the sum of the absolute values of the elements of a vector.
The L2 norm is the square root of the sum of the squares of the elements of a vector.
The MSE is the mean squared error.

## NumPy Arrays and PyTorch Tensors

NumPy is the most widely used library for scientific and numeric programming in Python. It provides similar functionality and a similar API to that provided by PyTorch; however, it does not support using the GPU or calculating gradients, which are both critical for deep learning. **NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure Python.**

A NumPy array is a multidimensional table of data, with all items of the same type. Since that can be any type at all, they can even be arrays of arrays, with the innermost arrays potentially being different sizes—this is called a jagged array.
A PyTorch tensor is nearly the same thing as a NumPy array, but with an additional restriction that unlocks additional capabilities. The restriction is that a tensor cannot use just any old type—it has to use a single basic numeric type for all components.

### Broadcasting

Scientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank.

## Computer Vision

### Cross-entropy loss(交叉熵损失)

Cross-entropy loss is a loss function, which has two benefits:

- It works even when our dependent variable has more than two categories.
- It results in faster and more reliable training.

### Softmax

Softmax function is a function that takes a vector of K real-valued scores and converts it into a vector of K probabilities that sum to 1.

```python
def softmax(x):
    return exp(x) / exp(x).sum(dim=1, keepdim=True)
```

### Log Likelihood

Log Likelihood is a function that takes a vector of K probabilities and converts it into a real-valued score.

### Discriminative Learning Rates

Use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers)

### Mixed-precision Training

This refers to using less-precise numbers (half-precision oating point, also called fp16) where possible during training.

### Multi-Label Classification

### One-Hot Encoding

Using a vector of 0s, with a 1 in each location that is represented in the data, to encode a list of integers.

### Training a State-of-the-Art Model

#### DataSets

- ImageNet
- CIFAR-10
- MNIST

#### Normalization

The normalization is done by subtracting the mean and dividing by the standard deviation.

#### Progressive Resizing

Gradually using larger and larger images as you train.
Spending most of the epochs training with small images helps training complete much faster.
Completing training using large images makes the final accuracy much higher.

#### Test Time Augmentation

During inference or validation, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.

#### Mixup

The mixup is a powerful data augmentation technique that can provide dramatically higher accuracy, especially when you don’t have much data and don’t have a pretrained model that was trained on data similar to your dataset.
Mixup works as follows, for each image:

1. Select another image from your dataset at random.
2. Pick a weight at random.
3. Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable.
4. Take a weighted average (with the same weight) of this image’s labels with your image’s labels; this will be your dependent variable.

**Note:** *Mixup is used for all leading results for trainings of >80 epochs, and for fewer epochs Mixup is not being used.*

#### Label Smoothing

The label smoothing technique is used to reduce the effect of class imbalance and make your training more robust, even if there is mislabeled data. The result will be a model that generalizes better at inference.

## Collaborative Filtering
