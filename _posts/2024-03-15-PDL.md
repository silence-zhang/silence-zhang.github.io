# Practical Deep Learning

Here's the table of contents:

- TOC
{:toc}

## Deep Learning Vocabulary

![p4](/images/Pastedimage20240315144530.png)
![p2](/images/WX20240326-103456@2x.png)

## Machine Learning(ML)

The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.

## Neural Network(NN)

A neural network is a system of artificial neurons that are connected to each other in a way that allows them to learn and recognize patterns.

![p1](/images/Pastedimage20240314180458.png)

- input layer
- hidden layer
- output layer

![p3](/images/WX20240418-115714@2x.png)
![p4](/images/WX20240418-115935@2x.png)

### Basic Neural Network Equation

Basic Neural Network Equation:

```python

def simple_net(xb):
    res = xb@w1 + b1
    res = res.max(tensor(0.0))
    res = res@w2 + b2
    return res

# takeing advantage of pyTorch
simple_net = nn.Sequential(
    nn.Linear(28*28,30),
    nn.ReLU(),
    nn.Linear(30,1)
)

```

### [Activation Function](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)

The **activation function** known as *transfer function* is another function that is part of the neural network, which has the purpose of providing non-linearity to the model. The idea is that without an activation function, we just have multiple linear functions of the form y=mx+b. However, a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data. By introducing a non-linearity in between the linear layers, this is no longer true. Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions. In fact, it can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights. This is known as the universal approximation theorem.

The Activation Functions can be basically divided into 2 types:

- Linear Activation Functions
- Non-Linear Activation Functions

### Jargon Recap

*Activations*: Numbers that are calculated (both by linear and nonlinear layers)
*Parameters*:  Numbers that are randomly initialized, and optimized (that is, the numbers that define the model)
The number of dimensions of a tensor is its rank. There are some special tensors:

- Rank-0: scalar (0-dimensional tensor)
- Rank-1: vector (1-dimensional tensor)
- Rank-2: matrix (2-dimensional tensor)

## Parallel Distributed Processing(PDP)

- A set of processing units
- A state of activation
- An output function for each unit
- A pattern of connectivity among units
- A propagation rule for propagating patterns of activities through the network of connectivities
- An activation rule for combining the inputs impinging on a unit with the current state of that unit to produce an output for the unit
- A learning rule whereby patterns of connectivity are modified by experience
- An environment within which the system must operate

## Stochastic Gradient Descent([SGD: 随机梯度下降](https://zh.d2l.ai/chapter_optimization/sgd.html))

Neural networks are special because they are highly flexible, which means they can solve an unusually wide range of problems just by finding the right weights. This is powerful, because stochastic gradient descent provides us a way to find those weight values automatically.

**Seven Steps in SGD:**

1. Initialize the parameters – Random values often work best.
1. Calculate the predictions – This is done on the training set, one mini-batch at a time.
1. Calculate the loss – The average loss over the minibatch is calculated
1. Calculate the gradients – this is an approximation of how the parameters need to change in order to minimize the loss function
1. Step the weights – update the parameters based on the calculated weights
1. Repeat the process
1. Stop – In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.

![p1](/images/WX20240322-120021@2x.png)

### Backward pass and Forward pass

The forward pass is the process of feeding data through the network, and the backward pass is the process of propagating errors back through the network.

#### backward pass

compute all the gradients of a given loss with respect to its parameters.

#### forward pass

compute the output of the model on a given input, based on the matrix products.

### Learning Rate and Stepping

The learning rate is often a number between 0.001 and 0.1. It’s the amount by which the weights are adjusted during each iteration of the training loop.
Once you’ve picked a learning rate, you can adjust your parameters using this simple function:

``` python
w -= w.grad * lr
```

This is known as stepping your parameters, using an optimization step.

![p](/images/WX20240322-151036@2x.png)

![p](/images/WX20240322-151049@2x.png)

![p](/images/WX20240322-151105@2x.png)

### The MNIST Loss Function

The loss function will return a value based on the given predictions and targets, where lower values correspond to better model predictions.

``` python
def mnist_loss(preds, targs):
    preds = preds.sigmoid()
    return torch.where(targs==1, 1-preds, preds).mean()
```

In plain English, this function will measure how distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it should be 0, and then it will take the mean of all those distances.

### Matrix Multiplication

In Python, matrix multiplication is represented with the @ operator.

``` python
Matrix multiplication in Python:
# A is a 2x3 matrix, B is a 3x4 matrix, C is a 2x4 matrix
C = A @ B
```

![p5](/images/WX20240325-144319@2x.png)

### Sigmoid Function

The sigmoid function always outputs a number between 0 and 1.

``` python
def sigmoid(x): return 1 / (1 + torch.exp(-x))
```

![p7](/images/WX20240325-151716.png)

### Rectified Linear Unit([ReLU](https://builtin.com/machine-learning/relu-activation-function))

The *Rectified Linear Unit* is a function that rectifies the output of a neuron. It is a non-linear activation function that is used in neural networks.

```python
# torch built-in function
plot_function(F.relu)
```

![p4](/images/WX20240325-175443.png)

### Mini-Batches

We calculate the average loss for a few data items at a time. This is called a mini-batch. The number of data items in the mini-batch is called the batch size.
The larger batch size, the more accurate the gradient descent, but it will take longer, and you will process fewer mini-batches per epoch.

### Gradient Accumulation

Gradient accumulation is a technique that simulates a larger batch size by accumulating gradients from multiple small batches before performing a weight update. This technique can be helpful in scenarios where the available memory is limited, and the batch size that can fit in memory is small.

### Optimizer

Original case:

```python

def mnist_loss(preds, targs):
    preds = preds.sigmoid()
    return torch.where(targs==1, 1-preds, preds).mean()

def calc_grad(xb, yb, model):
    preds = model(xb)
    loss = mnist_loss(preds, yb)
    loss.backward()

def batch_accuracy(xb, yb):
    preds = xb.sigmoid()
    correct = (preds > 0.5) == yb
    return correct.float().mean()

def validate_epoch(model):
    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]
    return round(torch.stack(accs).mean().item(), 4)

linear_model = nn.Linear(28*28,1)
class BasicOptim:
    def __init__(self, params, lr): self.params, self.lr = list(params), lr
    def step(self, *args, **kwargs):
        for p in self.params: p.data -= p.grad.data * self.lr
    def zero_grad(self, *args, **kwargs):
        for p in self.params: p.grad = None

opt = BasicOptim(linear_model.parameters(), lr)

def train_epoch(model):
    for xb, yb in dl:
        calc_grad(xb, yb, model)
        opt.step()
        opt.zero_grad()

def train_model(model, epochs):
    for i in range(epochs):
        train_epoch(model)
        print(validate_epoch(model), end=' ')

train_model(linear_model, 20)

```

SGD case:

```python
linear_model = nn.Linear(28*28, 1)
opt = SGD(linear_model.parameters(), lr)
train_model(linear_model, 20)
```

Learner case:

```python
dls = DataLoaders(dl, valid_dl)
learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)
learn.fit(10, lr = lr)

```

Network case:

```python
simple_net = nn.Sequential(
    nn.Linear(28*28,30),
    nn.ReLU(),
    nn.Linear(30,1)
)

learn = Learner(dls, simple_net, opt_func=SGD,
 loss_func=mnist_loss, metrics=batch_accuracy)

learn.fit(40,0.1)

```

At this point, we have something that is rather magical:

- A function that can solve any problem to any level of accuracy (the neural net‐work) given the correct set of parameters
- A way to find the best set of parameters for any function (stochastic gradient descent)

### [Momentum](http://zh.gluon.ai/chapter_optimization/momentum.html)

## Loss Functions

- nn.CrossEntropyLoss for single-label classification
- nn.BCEWithLogitsLoss for multi-label classification
- nn.MSELoss for regression

## Training Loop

![p2](/images/Pastedimage20240315111753.png)

## Feedback Loops

- A *`predictive policing`* model is created based on where arrests have been made in the past. In practice, this is not actually predicting crime, but rather predicting arrests, and is therefore partially simply reflecting biases in existing policing processes.
- Law enforcement officers then might use that model to decide where to focus their policing activity, resulting in increased arrests in those areas.
- Data on these additional arrests would then be fed back in to retrain future versions of the model.

## Classification and Regression

A classification model is one that attempts to predict a class, or category. That is, it’s predicting from a number of discrete possibilities, such as “dog” or “cat.”
A regression model is one that attempts to predict one or more numeric quantities, such as a temperature or a location.

## Overfitting and Underfitting

confirmed that overfitting is occurring (i.e., if you have observed the validation accuracy getting worse during training).

![p3](/images/Pastedimage20240315115211.png)

## Feature Engineering

## Convolutional Neural Network (CNN)

### [Basic Concepts](https://www.cnblogs.com/chumingqian/articles/11569397.html)

- **convolution**
- **kernel**
- **filter**
- **stride**
- **padding**

#### kernel

![p1](/images/WX20240419-104820@2x.png)

#### strides and padding

![p4](/images/WX20240419-152310@2x.png)

With a 5×5 input, 4×4 kernel, and 2 pixels of padding, and end up with a 6×6 activation map

![p5](/images/WX20240419-152418@2x.png)

![p6](/images/WX20240419-154155@2x.png)

#### convolutions

![p7](/images/WX20240419-154646@2x.png)

### Nested List Comprehensions

**Syntax:** new_list = [[expression for item in list] for item in list]

**Parameters:**

- **Expression:** Expression that is used to modify each item in the statement
- **Item:** The element in the iterable
- **List:** An iterable object

```python
# create matrix using nested list comprehensions
matrix = [[j for j in range(5)] for i in range(5)]
 
print(matrix)

```

### Convolution Arithmetic

#### Receptive Fields

The *receptive field* is the area of an image that is involved in the calculation of a layer.

#### [Pooling Layers](https://www.dremio.com/wiki/pooling-layers/#:~:text=Pooling%20Layers%20divide%20the%20input,representation%20of%20the%20input%20data.)

<span style="background-color: #FFFF00">
Pooling Layers divide the input data into small regions, called pooling windows or receptive fields, and perform an aggregation operation, such as taking the maximum or average value, within each window. This aggregation reduces the size of the feature maps, resulting in a compressed representation of the input data.
</span>

#### 1cycle training

#### Batch normalization(batchnorm)

taking an average of the mean and standard deviations of the activations of a layer and using those to normalize the
activations.

### Fully Convolutional Networks

[Fully Convolutional Networks](https://paperswithcode.com/method/fcn), or FCNs, are an architecture used mainly for semantic segmentation. They employ solely locally connected layers, such as convolution, pooling and upsampling. Avoiding the use of dense layers means less parameters (making the networks faster to train). It also means an FCN can work for variable image sizes given all connections are local.

### Transposed Convolution

A [transposed convolutional layer](https://www.geeksforgeeks.org/what-is-transposed-convolutional-layer/) is an upsampling layer that generates the output feature map greater than the input feature map.

![p1](/images/WX20240429-145430@2x.png)

## ResNet(Residual Network: 残差网络)

**reference:** https://arxiv.org/pdf/1512.03385

A simple residual network block:
![p3](/images/WX20240428-155757@2x.png)

### Skip Connections

Skip some layer in neural network and feeds the output of one layer as the input to the next layers.
There are two fundamental ways that we could use Skip Connections through different non-sequential layers:

- Addition as in residual architectures.
- Concatenation as in densely connected architecture.

### Identity Mapping(恒等映射)

Returning the input without changing it at all. This process is performed by an identity function(恒等函数).

### Bottleneck Layers

![p4](/images/WX20240428-175323@2x.png)

## Pre-trained Model

A model that has weights that have already been trained on another dataset is called a *pre‐trained* model.

## Transfer Model

Using a pretrained model for a task different from what it was originally trained for.

## Fine-Tuning(adapt a pre-trained model for a new dataset)

A transfer learning technique that updates the parameters of a pre‐ trained model by training for additional epochs using a different task from that used for pretraining.

## Training Sets && Validation Sets && Test Sets

- Training data is fully exposed
- Validation data is less exposed
- Test data is totally hidden

## Independent Variables && Dependent Variables

The independent variable is the thing we are using to make predictions from, and the dependent variable is our target.

## Data Augmentation

Data augmentation refers to creating random variations of our input data, such that they appear different but do not change the meaning of the data.

## Out-Of-Domain data

That is to say, there may be data that our model sees in production that is very different from what it saw during training.

## Domain Shift

This is when the type of data changes gradually over time. For example, an insurance company is using a deep learning model as part of their pricing algorithm, but over time their customers will be different, with the original training data not being representative of current data, and the deep learning model being applied on effectively out-of-domain data.

## Confusion Matrix

![p4](/images/Pastedimage20240320104843.png)

## Data Ethics

- *Recourse and accountability*(追责和问责)
- *Feedback loops*
- *Bias*
- *Disinformation*

## L1 norm and L2 norm and MSE(mean square error)

The L1 norm is the sum of the absolute values of the elements of a vector.
The L2 norm is the square root of the sum of the squares of the elements of a vector.
The MSE is the mean squared error.

## NumPy Arrays and PyTorch Tensors

NumPy is the most widely used library for scientific and numeric programming in Python. It provides similar functionality and a similar API to that provided by PyTorch; however, it does not support using the GPU or calculating gradients, which are both critical for deep learning. **NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure Python.**

A NumPy array is a multidimensional table of data, with all items of the same type. Since that can be any type at all, they can even be arrays of arrays, with the innermost arrays potentially being different sizes—this is called a jagged array.
A PyTorch tensor is nearly the same thing as a NumPy array, but with an additional restriction that unlocks additional capabilities. The restriction is that a tensor cannot use just any old type—it has to use a single basic numeric type for all components.

### Broadcasting

Scientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank.

## Computer Vision

### Cross-entropy loss(交叉熵损失)

Cross-entropy loss is a loss function, which has two benefits:

- It works even when our dependent variable has more than two categories.
- It results in faster and more reliable training.

### Softmax

Softmax function is a function that takes a vector of K real-valued scores and converts it into a vector of K probabilities that sum to 1.

```python
def softmax(x):
    return exp(x) / exp(x).sum(dim=1, keepdim=True)
```

### Log Likelihood

Log Likelihood is a function that takes a vector of K probabilities and converts it into a real-valued score.

### Discriminative Learning Rates

Use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers)

### Mixed-precision Training

This refers to using less-precise numbers (half-precision oating point, also called fp16) where possible during training.

### Multi-Label Classification

### One-Hot Encoding

Using a vector of 0s, with a 1 in each location that is represented in the data, to encode a list of integers.

### Training a State-of-the-Art Model

#### DataSets

- ImageNet
- CIFAR-10
- MNIST

#### Normalization

The normalization is done by subtracting the mean and dividing by the standard deviation.

#### Progressive Resizing

Gradually using larger and larger images as you train.
Spending most of the epochs training with small images helps training complete much faster.
Completing training using large images makes the final accuracy much higher.

#### Test Time Augmentation(TTA)

During inference or validation, creating multiple versions of each image using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.

#### Mixup

The mixup is a powerful data augmentation technique that can provide dramatically higher accuracy, especially when you don’t have much data and don’t have a pretrained model that was trained on data similar to your dataset.
Mixup works as follows, for each image:

1. Select another image from your dataset at random.
2. Pick a weight at random.
3. Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable.
4. Take a weighted average (with the same weight) of this image’s labels with your image’s labels; this will be your dependent variable.

**Note:** *Mixup is used for all leading results for trainings of >80 epochs, and for fewer epochs Mixup is not being used.*

#### Label Smoothing

The label smoothing technique is used to reduce the effect of class imbalance and make your training more robust, even if there is mislabeled data. The result will be a model that generalizes better at inference.

## Collaborative Filtering

### [Dot Product and Cross Product](https://zhuanlan.zhihu.com/p/359975221)

The mathematical operation of multiplying the elements of two vectors together, and then summing up the result. The dot product is a fundamental operation in machine learning.

The cross product is a vector operator that takes two vectors and returns a vector perpendicular to both of the input vectors.

### Latent factor

A latent factor is a factor that is not directly observed, but is inferred from the observed data.

### [Embedding](https://aws.amazon.com/cn/what-is/embeddings-in-machine-learning/?nc1=h_ls)

The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the embedding matrix.

### Embedding Distance

The embedding distance is the distance between two vectors in the embedding space, which is used to measure the similarity between two items.

![p1](/images/WX20240409-104447@2x.png)

### Weight Decay

Weight decay, also known as L2 regularization, is a regularization technique applied during the training of neural networks. It involves adding a penalty term to the loss function based on the squared magnitude of the weights.

### Probabilistic Matrix Factorization(PMF)

## Natural Language Processing

- Text Preprocessing
- Tokenization
- Numericalization
- Embedding Matrix

### Recurrent neural network(RNN)

A recurrent neural network (RNN) is a deep learning model that is trained to process and convert a sequential data input into a specific sequential data output. Sequential data is data—such as words, sentences, or time-series data—where sequential components interrelate based on complex semantics and syntax rules.

![p1](/images/WX20240418-145630@2x.png)
*Note: Each shape represents activations: rectangle for input, circle for hidden (inner) layer activations, and triangle for output activations.*

#### Hidden State (variable h)

The activations that are updated at each step of a recurrent neural network.

#### Backpropagation Through Time([BPTT](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html))

#### Gated Recurrent Units (GRUs)

#### Long Short-Term Memory(LSTM)

![p1](/images/WX20240418-165049@2x.png)

#### Regularizing an LSTM

- **Dropout**

![p1](/images/WX20240418-170824@2x.png)
It helps the neurons to cooperate better together;then it makes the activations more noisy, thus making the model more robust.

- **Activation Regularization and Temporal Activation Regularization**

- **Training a Weight-Tied Regularized LSTM**

### MultilayerRNNs

### Self-supervised learning

Training a model using labels that are embedded in the independent variable, rather than requiring external labels. For instance, training a model to predict the next word in a text.

### Encoder

The model not including the task-specific final layer(s) is called the encoder. This term means much the same thing as “body” when applied to vision CNNs, but “encoder” tends to be more used for NLP and generative models.

## Tabular Modeling Deep Dive

### Continuous and Categorical Variables

Continuous variables are numerical data, such as “age,” that can be directly fed to the model, since you can add and multiply them directly. Categorical variables contain a number of discrete levels, such as “movie ID,” for which addition and multiplication don’t have meaning (even if they’re stored as numbers).

### Decision Trees

Decision trees are a type of supervised learning algorithm that can be used for both regression and classification problems. They are a type of ensemble model, which means that they are made up of a number of decision trees that are trained independently and then combined to make a final prediction.

![decision trees](/images/WX20240409-174303@2x.png)

### Random Forests

Random forests are a type of ensemble model that are made up of a number of decision trees that are trained independently and then combined to make a final prediction.
In essence, a random forest is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters.

*Random forests* are the easiest to train, because they are extremely resilient to hyperparameter choices and require little preprocessing. They are fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods.

#### Out-of-Bag Error(OOB)

The OOB error is a way of measuring prediction error in the training dataset by including in the calculation of a row’s error trees only where that row was not included in training.

#### Data Leakage

Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.

In other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate.

### Neural Networks

*Neural networks* take the longest time to train and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting.

### Boosting

Here is how boosting works:

1. Train a small model that underfits your dataset.
2. Calculate the predictions in the training set for this model.
3. Subtract the predictions from the targets; these are called the residuals and represent the error for each point in the training set.
4. Go back to step 1, but instead of using the original targets, use the residuals as the targets for the training.
5. Continue doing this until you reach a stopping criterion, such as a maximum number of trees, or you observe your validation set error getting worse.

#### Gradient boosting machines(GBMs)

*Gradient boosting machines* in theory are just as fast to train as random forests, but in practice you will have to try lots of hyperparameters. They can overfit, but they are often a little more accurate than random forests.

#### Gradient boosted decision trees

#### XGBoost

## FastCore Mid-level API

### Transforms

```python
class NormalizeMean(Transform):
    def setups(self, items): self.mean = sum(items)/len(items)
    def encodes(self, x): return x-self.mean
    def decodes(self, x): return x+self.mean

tfm = NormalizeMean()
tfm.setups([1,2,3,4,5])
start = 2
tfm.decodes(start)
```

![p6](/images/WX20240418-104811@2x.png)

### Pipeline

## Stable Diffusion

### [CLIP](https://openai.com/index/clip/) (Contrastive Language–Image Pre-training)

### Forward Diffusion(Markov process with Gaussian transition)

### [VAE](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)(Variational Autoencoder)

VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data.

![p1](/images/WX20240620-165250@2x.png)

![p2](/images/WX20240620-174504@2x.png)

### [GANs](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)(Generative Adversarial Networks)

### U-Net

### Latent Diffusion

#### latent space
